/**
 * RAG (Retrieval-Augmented Generation) Service
 * Handles: getGroundedAnswer, getExpertInfoForCondition, getChatbotResponse
 */

import { getGeminiClient } from './core.service';
import { ragResponseSchema } from './schemas/rag.schemas';
import {
  RAG_ANSWER_PROMPT,
  CHATBOT_SYSTEM_INSTRUCTION,
  CHATBOT_PROMPT,
  CONDITION_INFO_PROMPT,
} from './prompts/rag.prompts';
import { findRelevantChunks } from '../../constants/knowledgeBase';
import { logger } from '../../config/logger.config';

// Type definitions (will be moved to types/ later)
export interface RagResult {
  answer: string;
  sources: Array<{
    sourceName: string;
    url: string;
  }>;
}

export interface ChatMessage {
  role: 'user' | 'model';
  text: string;
  image?: {
    base64: string;
    mimeType: string;
  };
  sources?: Array<{
    sourceName: string;
    url: string;
  }>;
}

/**
 * Get grounded answer using RAG (Retrieval-Augmented Generation)
 */
export const getGroundedAnswer = async (question: string): Promise<RagResult> => {
  try {
    const ai = getGeminiClient();
    const relevantChunks = findRelevantChunks(question);

    logger.info('Getting grounded answer', {
      question: question.substring(0, 50),
      relevantChunksCount: relevantChunks.length,
    });

    if (relevantChunks.length === 0) {
      logger.warn('No relevant chunks found for question');
      return {
        answer:
          'R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë√°ng tin c·∫≠y n√†o li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n trong c∆° s·ªü ki·∫øn th·ª©c c·ªßa m√¨nh. Vui l√≤ng th·ª≠ m·ªôt c√¢u h·ªèi kh√°c ho·∫∑c tham kh·∫£o √Ω ki·∫øn b√°c sƒ© da li·ªÖu.',
        sources: [],
      };
    }

    const prompt = RAG_ANSWER_PROMPT(question, relevantChunks);

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-pro',
      contents: prompt,
      config: {
        responseMimeType: 'application/json',
        responseSchema: ragResponseSchema,
      },
    });

    if (!response.text) {
      throw new Error('No response from Gemini');
    }

    const jsonText = response.text.trim();
    const result = JSON.parse(jsonText) as RagResult;

    logger.info('Grounded answer generated successfully', {
      sourcesCount: result.sources.length,
    });

    return result;
  } catch (error) {
    logger.error('Error getting grounded answer from Gemini:', error);
    throw new Error('Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i.');
  }
};

/**
 * Get expert information for specific condition
 */
export const getExpertInfoForCondition = async (condition: string): Promise<RagResult> => {
  const question = CONDITION_INFO_PROMPT(condition);
  logger.info('Getting expert info for condition', { condition });
  return getGroundedAnswer(question);
};

/**
 * Get chatbot response with RAG context
 */
export const getChatbotResponse = async (
  _history: ChatMessage[],
  text: string,
  image?: { base64: string; mimeType: string }
): Promise<ChatMessage> => {
  try {
    const ai = getGeminiClient();

    // RAG: Find relevant medical knowledge
    const relevantChunks = findRelevantChunks(text);
    const context =
      relevantChunks.length > 0
        ? 'D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë th√¥ng tin y khoa li√™n quan t·ª´ c∆° s·ªü ki·∫øn th·ª©c, h√£y s·ª≠ d·ª•ng n√≥ ƒë·ªÉ tr·∫£ l·ªùi n·∫øu ph√π h·ª£p:\n' +
          relevantChunks
            .map(
              (chunk, index) =>
                `Ngu·ªìn [${index}]:\n- Ngu·ªìn g·ªëc: ${chunk.source}\n- N·ªôi dung: ${chunk.content}`
            )
            .join('\n\n')
        : 'Kh√¥ng c√≥ th√¥ng tin y khoa n√†o trong c∆° s·ªü ki·∫øn th·ª©c ƒë∆∞·ª£c t√¨m th·∫•y li√™n quan tr·ª±c ti·∫øp.';

    const prompt = CHATBOT_PROMPT(text, context);

    logger.info('Getting chatbot response', {
      hasImage: !!image,
      relevantChunksCount: relevantChunks.length,
    });

    // Build parts array with proper typing
    const parts = [{ text: prompt }] as (
      | { text: string }
      | { inlineData: { mimeType: string; data: string } }
    )[];

    if (image) {
      parts.push({
        inlineData: { mimeType: image.mimeType, data: image.base64 },
      });
    }

    const response = await ai.models.generateContent({
      model: 'gemini-2.5-pro',
      contents: { parts },
      config: {
        systemInstruction: CHATBOT_SYSTEM_INSTRUCTION,
        responseMimeType: 'application/json',
        responseSchema: ragResponseSchema,
      },
    });

    if (!response.text) {
      throw new Error('No response from Gemini');
    }

    const jsonText = response.text.trim();
    const parsedResult = JSON.parse(jsonText) as RagResult;

    logger.info('Chatbot response generated successfully', {
      sourcesUsed: parsedResult.sources.length,
    });

    return {
      role: 'model',
      text: parsedResult.answer,
      sources: parsedResult.sources,
    };
  } catch (error) {
    logger.error('Error getting chatbot response:', error);
    return {
      role: 'model',
      text: 'Ui, tui b·ªã lag x√≠u r√πi ü•∫. B·ªì th·ª≠ l·∫°i sau nha!',
    };
  }
};
